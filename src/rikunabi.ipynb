{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71422d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "from http.client import RemoteDisconnected\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import html5lib\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e9765d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target url base list and file name list for each target occupation\n",
    "urls = [\n",
    "    'https://next.rikunabi.com/kw%91%E5%8DH/?keywordsearch=1&log_f=1',\n",
    "    'https://next.rikunabi.com/kw%92%CA%90M+%8DH%8E%96/?keywordsearch=1&log_f=1',\n",
    "    'https://next.rikunabi.com/kw%83%255C%83t%83g%83E%83F%83A+%8AJ%94%AD/?keywordsearch=1&log_f=1',\n",
    "    'https://next.rikunabi.com/kw%96%F2%8D%DC%8Et/?keywordsearch=1&log_f=1',\n",
    "    'https://next.rikunabi.com/kw%8E%F3%95t+%88%C4%93%E0/?keywordsearch=1&log_f=1',\n",
    "    'https://next.rikunabi.com/kw%8A%C5%8C%EC%8Et/?keywordsearch=1&log_f=1',\n",
    "    'https://next.rikunabi.com/kw%8Cx%94%F5/?keywordsearch=1&log_f=1',\n",
    "    'https://next.rikunabi.com/kw%94%FC%97e%8Et/?keywordsearch=1&log_f=1'\n",
    "    ]\n",
    "fileNames = [\n",
    "    'carpentor',\n",
    "    'electrician',\n",
    "    'software_engineer',\n",
    "    'pharmacist',\n",
    "    'receptionist',\n",
    "    'nurse',\n",
    "    'security_guard',\n",
    "    'beautician'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c08077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set className to extract the data from the html and url if not finding on the first page \n",
    "jobNumberFinder = 'rnn-pageNumber rnn-textXl'\n",
    "jobTitleFinder = \"rnn-linkText rnn-linkText--black\"\n",
    "jobDetailBaseUrl = \"https://next.rikunabi.com\"\n",
    "jobDetailDescriptionFinder = 'rn3-companyOfferRecruitment__headText'\n",
    "jobDescriptionAndWantedEmployeeFinder = '.rn3-companyOfferRecruitment__text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8428712",
   "metadata": {},
   "outputs": [],
   "source": [
    "## main process of web scraping \n",
    "fileNameindex = 0\n",
    "failedTopUrls = []\n",
    "failedDetailUrls = []\n",
    "errors = []\n",
    "# loop for each url per occupation\n",
    "for url in urls:\n",
    "    htmlNumber = 1\n",
    "    data = []\n",
    "    dataIndex = 1\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html5lib\")\n",
    "        time.sleep(2)\n",
    "        response.close()\n",
    "        ## get total number of job advertisements for the occupation \n",
    "        jobNumber = soup.find(\"span\", class_=jobNumberFinder).get_text()\n",
    "        print(\"Success - url:\" + url + \"jobNumber:\" + jobNumber)\n",
    "    except (RemoteDisconnected, ConnectionError, Exception) as e:\n",
    "        print('Fail - url:' + url)\n",
    "        failedTopUrls.append(url)\n",
    "        errors.append(e)\n",
    "        time.sleep(0.3)\n",
    "        continue\n",
    "    while(True):\n",
    "        if(htmlNumber == 1):\n",
    "            titles = soup.find_all(\"a\", class_=jobTitleFinder)\n",
    "        else:\n",
    "            try:\n",
    "                newUrl = url.replace('(?<=\\?)(.*)(?=\\/)', 'crn' + str(htmlNumber) + '.html')\n",
    "                response = requests.get(newUrl)                \n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(response.content, \"html5lib\")\n",
    "                response.close()\n",
    "                titles = soup.find_all(\"a\", class_=jobTitleFinder)\n",
    "            except (RemoteDisconnected, ConnectionError, Exception) as e:\n",
    "                print('Fail - url:' + newUrl)\n",
    "                errors.append(e)\n",
    "                failedTopUrls.append(newUrl)\n",
    "                htmlNumber = htmlNumber + 50\n",
    "                if(htmlNumber - 1 >= int(jobNumber)):\n",
    "                    break\n",
    "                time.sleep(random.randint(2,5))\n",
    "                continue\n",
    "        for title in titles:\n",
    "            try:\n",
    "                jobDetailResponse = requests.get(jobDetailBaseUrl + title['href'])                \n",
    "                time.sleep(2)\n",
    "                jobDetailSoup = BeautifulSoup(jobDetailResponse.content, \"html5lib\")\n",
    "                jobDetailResponse.close()\n",
    "                jobDescriptionSummary = None if jobDetailSoup.find(\"div\", class_=jobDetailDescriptionFinder) is None else jobDetailSoup.find(\"div\", class_=jobDetailDescriptionFinder).get_text()\n",
    "                if jobDescriptionSummary is not None:\n",
    "                    jobDescriptions = jobDetailSoup.select(jobDescriptionAndWantedEmployeeFinder)\n",
    "                    jobDescriptionText = ''\n",
    "                    jobWantedPersonText = ''\n",
    "                    descriptionIndex = 0\n",
    "                    for jobDescription in jobDescriptions:\n",
    "                        if(descriptionIndex == 0):\n",
    "                            jobDescriptionText = jobDescription.get_text()\n",
    "                        elif(descriptionIndex == 1):\n",
    "                            jobWantedPersonText = jobDescription.get_text()\n",
    "                            break\n",
    "                        descriptionIndex = descriptionIndex + 1                    \n",
    "            except (RemoteDisconnected, ConnectionError, Exception) as e:\n",
    "                print('url:' + jobDetailBaseUrl + title['href'])\n",
    "                failedDetailUrls.append(newUrl)\n",
    "                errors.append(e)\n",
    "                continue\n",
    "            if jobDescriptionSummary is None:\n",
    "                try:\n",
    "                    jobMoreDetailResponse = requests.get(jobDetailBaseUrl + title['href'].replace('nx1', 'nx2'))\n",
    "                    time.sleep(2)\n",
    "                    jobDetailMoreSoup = BeautifulSoup(jobMoreDetailResponse.content, \"html5lib\")\n",
    "                    jobMoreDetailResponse.close()                \n",
    "                    jobDescriptionSummary = jobDetailMoreSoup.find(\"div\", class_=jobDetailDescriptionFinder).get_text()\n",
    "                    if jobDescriptionSummary is not None:\n",
    "                        jobDescriptions = jobDetailMoreSoup.select(jobDescriptionAndWantedEmployeeFinder)\n",
    "                        jobDescriptionText = ''\n",
    "                        jobWantedPersonText = ''\n",
    "                        descriptionIndex = 0                        \n",
    "                        for jobDescription in jobDescriptions:\n",
    "                            if(descriptionIndex == 0):\n",
    "                                jobDescriptionText = jobDescription.get_text()\n",
    "                            elif(descriptionIndex == 1):\n",
    "                                jobWantedPersonText = jobDescription.get_text()\n",
    "                                break\n",
    "                            descriptionIndex = descriptionIndex + 1                    \n",
    "                except (RemoteDisconnected, ConnectionError, Exception) as e:\n",
    "                    print('url:' + jobDetailBaseUrl + title['href'].replace('nx1', 'nx2'))\n",
    "                    errors.append(e)\n",
    "                    continue\n",
    "                \n",
    "            data.append(\n",
    "                {str(dataIndex) : {\n",
    "                    'title': title.get_text(),\n",
    "                    'description_summary':jobDescriptionSummary,\n",
    "                    'description_detail':jobDescriptionText,\n",
    "                    'desired_person':jobWantedPersonText\n",
    "                    }})\n",
    "            dataIndex = dataIndex + 1\n",
    "            time.sleep(random.randint(2,5))\n",
    "        htmlNumber = htmlNumber + 50\n",
    "        if(htmlNumber - 1 >= int(jobNumber)):\n",
    "            break\n",
    "        time.sleep(random.randint(3,5))\n",
    "    \n",
    "    # export data into json file\n",
    "    with open('./data/job_advertisements/ja/' + fileNames[fileNameindex] + '.json', mode='w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "    print(fileNames[fileNameindex] + ':done')\n",
    "    fileNameindex = fileNameindex + 1\n",
    "    \n",
    "# export failed search top url data into json file\n",
    "with open('./data/job_advertisements/ja/failedTopUrls.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(failedTopUrls, file, ensure_ascii=False, indent=2)\n",
    "# export failed job detail url data into json file\n",
    "with open('./data/job_advertisements/ja/failedDetailUrls.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(failedDetailUrls, file, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
