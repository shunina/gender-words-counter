{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8ad3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import html5lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=carpenter',\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=driver',\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=electrician',\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=software+engineer',\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=pharmacist',\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=secretary',\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=nurse',\n",
    "    'https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=childminder',\n",
    "]\n",
    "\n",
    "fileNames = [\n",
    "    'carpentor',\n",
    "    'driver',\n",
    "    'electrician',\n",
    "    'software_engineer',\n",
    "    'pharmacist',\n",
    "    'secretary',\n",
    "    'nurse',\n",
    "    'childminder'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobNumberFinder = 'govuk-heading-l'\n",
    "jobTitleFinder = \"govuk-heading-s govuk-!-margin-top-4 govuk-!-margin-bottom-2\"\n",
    "jobDetailDescriptionFinder = 'govuk-body govuk-!-margin-bottom-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - url:https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=nursejobNumber:5000\n",
      "Success - dump nurse.json\n",
      "Success - url:https://findajob.dwp.gov.uk/search?loc=86383&p=1&pp=50&q=childminderjobNumber:63\n",
      "Success - dump childminder.json\n",
      "Success - dump failedTopUrls.json\n",
      "Success - dump failedDetailUrls.json\n"
     ]
    }
   ],
   "source": [
    "fileNameIndex = 0\n",
    "failedTopUrls = []\n",
    "failedDetailUrls = []\n",
    "pageNumber = 1\n",
    "for url in urls:\n",
    "    data = []\n",
    "    dataIndex = 1\n",
    "    pageNumber = 1\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html5lib\")\n",
    "        time.sleep(0.5)\n",
    "        response.close()\n",
    "        jobNumberText = soup.find(\"h1\", class_=jobNumberFinder).get_text()\n",
    "        jobNumber = re.sub(r'\\D', '', jobNumberText) if jobNumberText is not None else 0\n",
    "        jobNumber = '5000' if int(jobNumber) > 5000 else jobNumber\n",
    "        print(\"Success - url:\" + url + \"jobNumber:\" + jobNumber)\n",
    "    except ConnectionError:\n",
    "        print('Fail - url:' + url)\n",
    "        failedTopUrls.append(url)\n",
    "        time.sleep(0.5)\n",
    "        continue\n",
    "    while(True):\n",
    "        if(pageNumber == 1):\n",
    "            titles = []\n",
    "            titleAreas = soup.find_all(\"h3\", class_=jobTitleFinder)\n",
    "            for titleArea in titleAreas:\n",
    "                titles.append(titleArea.findChildren(\"a\" , recursive=False))\n",
    "        else:\n",
    "            try:\n",
    "                newUrl = url.replace('&p\\=[0-9]', '&p=' + str(pageNumber))\n",
    "                response = requests.get(newUrl)\n",
    "                soup = BeautifulSoup(response.content, \"html5lib\")\n",
    "                time.sleep(0.5)\n",
    "                response.close()\n",
    "                titles = []\n",
    "                titleAreas = soup.find_all(\"h3\", class_=jobTitleFinder)\n",
    "                for titleArea in titleAreas:\n",
    "                    titles.append(titleArea.findChildren(\"a\" , recursive=False))\n",
    "            except ConnectionError:\n",
    "                print('Fail - url:' + newUrl)\n",
    "                failedTopUrls.append(url)\n",
    "                pageNumber = pageNumber + 1\n",
    "                if(int(jobNumber) >= pageNumber * 50):\n",
    "                    break\n",
    "                time.sleep(0.5)\n",
    "                continue\n",
    "        for title in titles:\n",
    "            try:\n",
    "                jobDetailResponse = requests.get(title[0]['href'])\n",
    "                jobDetailSoup = BeautifulSoup(jobDetailResponse.content, \"html5lib\")\n",
    "                time.sleep(0.5)\n",
    "                jobDetailResponse.close()\n",
    "                jobdescription = None if jobDetailSoup.find(\"div\", class_=jobDetailDescriptionFinder) is None else jobDetailSoup.find(\"div\", class_=jobDetailDescriptionFinder).get_text()\n",
    "                data.append({str(dataIndex) : {'title': title[0].get_text(), 'description':jobdescription}})\n",
    "                dataIndex = dataIndex + 1\n",
    "            except ConnectionError:\n",
    "                print('Fail - detail url:' + url)\n",
    "                failedDetailUrls.append(title[0]['href'])\n",
    "                time.sleep(0.5)\n",
    "                continue\n",
    "            time.sleep(random.randint(2,7))\n",
    "        pageNumber = pageNumber + 1\n",
    "        if(pageNumber * 50 >= int(jobNumber)):\n",
    "            break\n",
    "        time.sleep(random.randint(2,7))\n",
    "    \n",
    "    # export data into json file\n",
    "    with open('./data/en/' + fileNames[fileNameIndex] + '.json', mode='w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "        print(\"Success - dump \" + fileNames[fileNameIndex] + '.json')\n",
    "    fileNameIndex = fileNameIndex + 1\n",
    "\n",
    "# export Top Url data into json file\n",
    "with open('./data/en/failedTopUrls.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(failedTopUrls, file, ensure_ascii=False, indent=2)\n",
    "    print(\"Success - dump \" + 'failedTopUrls.json')\n",
    "    \n",
    "# export detailed Url data into json file\n",
    "with open('./data/en/failedDetailUrls.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(failedDetailUrls, file, ensure_ascii=False, indent=2)\n",
    "    print(\"Success - dump \" + 'failedDetailUrls.json')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d40e36e7bb790554a55ec48c8df0d607bce49ede120479d7ccb8bf89d74e1e6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
